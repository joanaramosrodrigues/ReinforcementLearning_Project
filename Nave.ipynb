{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d95b7eb",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project â€“ Lunar Lander (Continuous Environment)\n",
    "\n",
    "## Environment Overview\n",
    "\n",
    "This notebook tackles the **LunarLanderContinuous-v2** environment, where the agent must learn to land a lunar module smoothly on a designated landing pad using continuous control. The complexity of this task comes from its continuous state and action spaces, requiring algorithms capable of function approximation and gradient-based learning.\n",
    "\n",
    "### Environment Characteristics:\n",
    "- **Observation space**: Continuous 8-dimensional vector  \n",
    "  (position, velocity, angle, angular velocity, and leg contact flags)\n",
    "- **Action space**: Continuous 2-dimensional vector  \n",
    "  (main engine and side engine thrusts in the range [-1, 1])\n",
    "\n",
    "## Algorithms Implemented\n",
    "\n",
    "We implement and compare two algorithms for continuous action environments:\n",
    "1. **DQN** â€“  \n",
    "2. **Sarsa** â€“ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e448d",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <strong style=\"display: block; margin-bottom: 10px;\">Group ??</strong> \n",
    "    <table style=\"margin: 0 auto; border-collapse: collapse; border: 1px solid black;\">\n",
    "        <tr>\n",
    "            <th style=\"border: 1px solid white; padding: 8px;\">Name</th>\n",
    "            <th style=\"border: 1px solid white; padding: 8px;\">Student ID</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Joana Rodrigues</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20240603</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Mara SimÃµes</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20240326</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Matilde Street</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20240523</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">TomÃ¡s Luzia</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20230477</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874db35",
   "metadata": {},
   "source": [
    "### ðŸ”— Table of Contents <a id='table-of-contents'></a>\n",
    "1. [Imports](#imports)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8d66d",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c74ef",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054e2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Importing necessary libraries for DQN implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc9342",
   "metadata": {},
   "source": [
    "**Load Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3b9fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Example of a state: [-0.0027173   1.399282   -0.27525324 -0.51725024  0.00315551  0.06234898\n",
      "  0.          0.        ]\n",
      "Action space: Discrete(4)\n",
      "Number of possible actions: 4\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=None)  # use render_mode=\"human\" to visualize the environment\n",
    "\n",
    "# Observe the state and action spaces\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Example of a state:\", env.reset()[0])  # [0] because reset() returns (observation, info)\n",
    "\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Number of possible actions:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8387cc",
   "metadata": {},
   "source": [
    "# 2. DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3882210",
   "metadata": {},
   "source": [
    "## 2.1. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa6a3e",
   "metadata": {},
   "source": [
    "We define a fully connected neural network to approximate the Q-function. \n",
    "The input is the 8-dimensional state from the environment, and the output is a vector of 4 Q-values, one for each possible action.\n",
    "This network will be used to predict the value of each action given a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e290968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_values = self.out(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db83658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "state_dim = env.observation_space.shape[0]  # should be 8\n",
    "action_dim = env.action_space.n             # should be 4\n",
    "q_net = QNetwork(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65580118",
   "metadata": {},
   "source": [
    "## 2.2. Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d254c3b",
   "metadata": {},
   "source": [
    "We implement a replay buffer to store past transitions (state, action, reward, next_state, done).\n",
    "During training, the agent samples random mini-batches from this buffer to break temporal correlations and stabilize learning.\n",
    "This mechanism is essential for Deep Q-Learning to resemble supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7210413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc93617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(capacity=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd400f63",
   "metadata": {},
   "source": [
    "## 2.3. Epsilon-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daae8d4",
   "metadata": {},
   "source": [
    "We implement an epsilon-greedy action selection strategy to balance exploration and exploitation:\n",
    "- With probability Îµ, the agent selects a random action (exploration).\n",
    "- With probability 1âˆ’Îµ, it selects the action with the highest predicted Q-value (exploitation).\n",
    "\n",
    "This allows the agent to discover new strategies while gradually learning to exploit the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56cb5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, q_network, epsilon, action_dim):\n",
    "    \"\"\"Selects an action using epsilon-greedy policy.\"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: random action\n",
    "        return np.random.randint(action_dim)\n",
    "    else:\n",
    "        # Exploit: choose best action based on Q-values\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # shape (1, 8)\n",
    "        q_values = q_network(state_tensor)\n",
    "        return torch.argmax(q_values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2169c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected action: 1\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "epsilon = 0.3\n",
    "\n",
    "action = select_action(state, q_net, epsilon, action_dim)\n",
    "print(\"Selected action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b2ead",
   "metadata": {},
   "source": [
    "## 2.4. Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede8b8a",
   "metadata": {},
   "source": [
    "We create a separate target network to provide stable Q-value targets during training.\n",
    "Initially, the target network is a copy of the main Q-network\n",
    "Throughout training, it is updated periodically to reflect the weights of the current Q-network.\n",
    "This technique reduces oscillations and helps stabilize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31dd66be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (out): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the target Q-network\n",
    "target_q_net = QNetwork(state_dim, action_dim)\n",
    "target_q_net.load_state_dict(q_net.state_dict())  # Copy weights from q_net\n",
    "target_q_net.eval()  # Set to evaluation mode (no dropout, no batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a42dd",
   "metadata": {},
   "source": [
    "## 2.5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d0e16",
   "metadata": {},
   "source": [
    "We implement the main training loop where the agent interacts with the environment and stores transitions in the replay buffer.\n",
    "Mini-batches are sampled to update the Q-network using target Q-values computed from a separate target network.\n",
    "The network is trained by minimizing the mean squared error between the predicted Q-values and the target Q-values, and the target network is periodically updated to stabilize training.\n",
    "Throughout training, we log the total reward per episode and track the value of epsilon to monitor the exploration-exploitation trade-off.\n",
    "At the end, both reward and epsilon histories are saved to text files for visualization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcf829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -409.47, Epsilon: 0.995\n",
      "Episode 1, Reward: -139.40, Epsilon: 0.990\n",
      "Episode 2, Reward: -141.39, Epsilon: 0.985\n",
      "Episode 3, Reward: -63.63, Epsilon: 0.980\n",
      "Episode 4, Reward: -151.33, Epsilon: 0.975\n",
      "Episode 5, Reward: -82.10, Epsilon: 0.970\n",
      "Episode 6, Reward: -77.75, Epsilon: 0.966\n",
      "Episode 7, Reward: -400.08, Epsilon: 0.961\n",
      "Episode 8, Reward: -133.35, Epsilon: 0.956\n",
      "Episode 9, Reward: -189.06, Epsilon: 0.951\n",
      "Episode 10, Reward: -280.30, Epsilon: 0.946\n",
      "Episode 11, Reward: -49.82, Epsilon: 0.942\n",
      "Episode 12, Reward: -103.97, Epsilon: 0.937\n",
      "Episode 13, Reward: -138.88, Epsilon: 0.932\n",
      "Episode 14, Reward: -125.65, Epsilon: 0.928\n",
      "Episode 15, Reward: -300.43, Epsilon: 0.923\n",
      "Episode 16, Reward: -125.84, Epsilon: 0.918\n",
      "Episode 17, Reward: -377.64, Epsilon: 0.914\n",
      "Episode 18, Reward: -45.60, Epsilon: 0.909\n",
      "Episode 19, Reward: -75.20, Epsilon: 0.905\n",
      "Episode 20, Reward: -79.02, Epsilon: 0.900\n",
      "Episode 21, Reward: -274.18, Epsilon: 0.896\n",
      "Episode 22, Reward: -135.37, Epsilon: 0.891\n",
      "Episode 23, Reward: -93.46, Epsilon: 0.887\n",
      "Episode 24, Reward: -73.06, Epsilon: 0.882\n",
      "Episode 25, Reward: -109.51, Epsilon: 0.878\n",
      "Episode 26, Reward: -147.24, Epsilon: 0.873\n",
      "Episode 27, Reward: -126.67, Epsilon: 0.869\n",
      "Episode 28, Reward: -40.23, Epsilon: 0.865\n",
      "Episode 29, Reward: -88.39, Epsilon: 0.860\n",
      "Episode 30, Reward: -91.45, Epsilon: 0.856\n",
      "Episode 31, Reward: -283.66, Epsilon: 0.852\n",
      "Episode 32, Reward: -123.90, Epsilon: 0.848\n",
      "Episode 33, Reward: -89.80, Epsilon: 0.843\n",
      "Episode 34, Reward: -74.58, Epsilon: 0.839\n",
      "Episode 35, Reward: -92.80, Epsilon: 0.835\n",
      "Episode 36, Reward: -109.27, Epsilon: 0.831\n",
      "Episode 37, Reward: -107.65, Epsilon: 0.827\n",
      "Episode 38, Reward: -97.33, Epsilon: 0.822\n",
      "Episode 39, Reward: -109.86, Epsilon: 0.818\n",
      "Episode 40, Reward: -89.91, Epsilon: 0.814\n",
      "Episode 41, Reward: -94.10, Epsilon: 0.810\n",
      "Episode 42, Reward: -74.55, Epsilon: 0.806\n",
      "Episode 43, Reward: -45.45, Epsilon: 0.802\n",
      "Episode 44, Reward: -108.33, Epsilon: 0.798\n",
      "Episode 45, Reward: 3.42, Epsilon: 0.794\n",
      "Episode 46, Reward: -42.41, Epsilon: 0.790\n",
      "Episode 47, Reward: -119.58, Epsilon: 0.786\n",
      "Episode 48, Reward: -207.40, Epsilon: 0.782\n",
      "Episode 49, Reward: -119.28, Epsilon: 0.778\n",
      "Episode 50, Reward: -84.09, Epsilon: 0.774\n",
      "Episode 51, Reward: -150.36, Epsilon: 0.771\n",
      "Episode 52, Reward: -178.13, Epsilon: 0.767\n",
      "Episode 53, Reward: -107.37, Epsilon: 0.763\n",
      "Episode 54, Reward: -114.81, Epsilon: 0.759\n",
      "Episode 55, Reward: -116.02, Epsilon: 0.755\n",
      "Episode 56, Reward: -108.20, Epsilon: 0.751\n",
      "Episode 57, Reward: -151.47, Epsilon: 0.748\n",
      "Episode 58, Reward: -48.41, Epsilon: 0.744\n",
      "Episode 59, Reward: -157.68, Epsilon: 0.740\n",
      "Episode 60, Reward: -112.65, Epsilon: 0.737\n",
      "Episode 61, Reward: -81.15, Epsilon: 0.733\n",
      "Episode 62, Reward: -74.20, Epsilon: 0.729\n",
      "Episode 63, Reward: -138.02, Epsilon: 0.726\n",
      "Episode 64, Reward: -57.15, Epsilon: 0.722\n",
      "Episode 65, Reward: -73.00, Epsilon: 0.718\n",
      "Episode 66, Reward: -150.56, Epsilon: 0.715\n",
      "Episode 67, Reward: -81.58, Epsilon: 0.711\n",
      "Episode 68, Reward: -83.26, Epsilon: 0.708\n",
      "Episode 69, Reward: -69.98, Epsilon: 0.704\n",
      "Episode 70, Reward: -60.29, Epsilon: 0.701\n",
      "Episode 71, Reward: -38.41, Epsilon: 0.697\n",
      "Episode 72, Reward: -86.19, Epsilon: 0.694\n",
      "Episode 73, Reward: -111.57, Epsilon: 0.690\n",
      "Episode 74, Reward: -65.27, Epsilon: 0.687\n",
      "Episode 75, Reward: -146.24, Epsilon: 0.683\n",
      "Episode 76, Reward: -103.92, Epsilon: 0.680\n",
      "Episode 77, Reward: -102.82, Epsilon: 0.676\n",
      "Episode 78, Reward: -99.61, Epsilon: 0.673\n",
      "Episode 79, Reward: -61.84, Epsilon: 0.670\n",
      "Episode 80, Reward: -60.18, Epsilon: 0.666\n",
      "Episode 81, Reward: -133.54, Epsilon: 0.663\n",
      "Episode 82, Reward: -100.64, Epsilon: 0.660\n",
      "Episode 83, Reward: -46.39, Epsilon: 0.656\n",
      "Episode 84, Reward: -46.85, Epsilon: 0.653\n",
      "Episode 85, Reward: -69.34, Epsilon: 0.650\n",
      "Episode 86, Reward: -45.38, Epsilon: 0.647\n",
      "Episode 87, Reward: -18.48, Epsilon: 0.643\n",
      "Episode 88, Reward: -55.23, Epsilon: 0.640\n",
      "Episode 89, Reward: -24.90, Epsilon: 0.637\n",
      "Episode 90, Reward: -80.87, Epsilon: 0.634\n",
      "Episode 91, Reward: -79.66, Epsilon: 0.631\n",
      "Episode 92, Reward: -89.16, Epsilon: 0.627\n",
      "Episode 93, Reward: -82.26, Epsilon: 0.624\n",
      "Episode 94, Reward: -45.93, Epsilon: 0.621\n",
      "Episode 95, Reward: -56.80, Epsilon: 0.618\n",
      "Episode 96, Reward: -20.79, Epsilon: 0.615\n",
      "Episode 97, Reward: -204.21, Epsilon: 0.612\n",
      "Episode 98, Reward: -82.69, Epsilon: 0.609\n",
      "Episode 99, Reward: -47.72, Epsilon: 0.606\n",
      "Episode 100, Reward: -127.77, Epsilon: 0.603\n",
      "Episode 101, Reward: -45.86, Epsilon: 0.600\n",
      "Episode 102, Reward: -96.42, Epsilon: 0.597\n",
      "Episode 103, Reward: -59.19, Epsilon: 0.594\n",
      "Episode 104, Reward: -35.19, Epsilon: 0.591\n",
      "Episode 105, Reward: -60.07, Epsilon: 0.588\n",
      "Episode 106, Reward: -105.40, Epsilon: 0.585\n",
      "Episode 107, Reward: -75.80, Epsilon: 0.582\n",
      "Episode 108, Reward: -75.52, Epsilon: 0.579\n",
      "Episode 109, Reward: -68.80, Epsilon: 0.576\n",
      "Episode 110, Reward: -50.66, Epsilon: 0.573\n",
      "Episode 111, Reward: -47.14, Epsilon: 0.570\n",
      "Episode 112, Reward: -4.86, Epsilon: 0.568\n",
      "Episode 113, Reward: -87.52, Epsilon: 0.565\n",
      "Episode 114, Reward: -100.54, Epsilon: 0.562\n",
      "Episode 115, Reward: -112.13, Epsilon: 0.559\n",
      "Episode 116, Reward: -92.91, Epsilon: 0.556\n",
      "Episode 117, Reward: -122.06, Epsilon: 0.554\n",
      "Episode 118, Reward: -76.29, Epsilon: 0.551\n",
      "Episode 119, Reward: -22.32, Epsilon: 0.548\n",
      "Episode 120, Reward: -50.79, Epsilon: 0.545\n",
      "Episode 121, Reward: -237.29, Epsilon: 0.543\n",
      "Episode 122, Reward: -62.46, Epsilon: 0.540\n",
      "Episode 123, Reward: -61.89, Epsilon: 0.537\n",
      "Episode 124, Reward: -23.11, Epsilon: 0.534\n",
      "Episode 125, Reward: -89.79, Epsilon: 0.532\n",
      "Episode 126, Reward: -121.80, Epsilon: 0.529\n",
      "Episode 127, Reward: -56.12, Epsilon: 0.526\n",
      "Episode 128, Reward: -25.72, Epsilon: 0.524\n",
      "Episode 129, Reward: 11.85, Epsilon: 0.521\n",
      "Episode 130, Reward: -91.77, Epsilon: 0.519\n",
      "Episode 131, Reward: -53.34, Epsilon: 0.516\n",
      "Episode 132, Reward: -16.21, Epsilon: 0.513\n",
      "Episode 133, Reward: -52.83, Epsilon: 0.511\n",
      "Episode 134, Reward: -79.69, Epsilon: 0.508\n",
      "Episode 135, Reward: 28.59, Epsilon: 0.506\n",
      "Episode 136, Reward: 5.56, Epsilon: 0.503\n",
      "Episode 137, Reward: -77.10, Epsilon: 0.501\n",
      "Episode 138, Reward: -65.35, Epsilon: 0.498\n",
      "Episode 139, Reward: 13.89, Epsilon: 0.496\n",
      "Episode 140, Reward: -68.37, Epsilon: 0.493\n",
      "Episode 141, Reward: 7.31, Epsilon: 0.491\n",
      "Episode 142, Reward: -94.56, Epsilon: 0.488\n",
      "Episode 143, Reward: -77.32, Epsilon: 0.486\n",
      "Episode 144, Reward: -95.24, Epsilon: 0.483\n",
      "Episode 145, Reward: -40.94, Epsilon: 0.481\n",
      "Episode 146, Reward: -48.55, Epsilon: 0.479\n",
      "Episode 147, Reward: -50.18, Epsilon: 0.476\n",
      "Episode 148, Reward: -70.43, Epsilon: 0.474\n",
      "Episode 149, Reward: -55.03, Epsilon: 0.471\n",
      "Episode 150, Reward: 207.76, Epsilon: 0.469\n",
      "Episode 151, Reward: -20.42, Epsilon: 0.467\n",
      "Episode 152, Reward: -55.24, Epsilon: 0.464\n",
      "Episode 153, Reward: -117.73, Epsilon: 0.462\n",
      "Episode 154, Reward: -27.17, Epsilon: 0.460\n",
      "Episode 155, Reward: 13.59, Epsilon: 0.458\n",
      "Episode 156, Reward: -52.11, Epsilon: 0.455\n",
      "Episode 157, Reward: 1.96, Epsilon: 0.453\n",
      "Episode 158, Reward: 3.14, Epsilon: 0.451\n",
      "Episode 159, Reward: -37.55, Epsilon: 0.448\n",
      "Episode 160, Reward: 63.92, Epsilon: 0.446\n",
      "Episode 161, Reward: -64.45, Epsilon: 0.444\n",
      "Episode 162, Reward: -41.77, Epsilon: 0.442\n",
      "Episode 163, Reward: -12.91, Epsilon: 0.440\n",
      "Episode 164, Reward: 5.21, Epsilon: 0.437\n",
      "Episode 165, Reward: 8.11, Epsilon: 0.435\n",
      "Episode 166, Reward: -88.68, Epsilon: 0.433\n",
      "Episode 167, Reward: -90.60, Epsilon: 0.431\n",
      "Episode 168, Reward: 23.12, Epsilon: 0.429\n",
      "Episode 169, Reward: -134.60, Epsilon: 0.427\n",
      "Episode 170, Reward: -66.00, Epsilon: 0.424\n",
      "Episode 171, Reward: 35.96, Epsilon: 0.422\n",
      "Episode 172, Reward: -53.62, Epsilon: 0.420\n",
      "Episode 173, Reward: 33.69, Epsilon: 0.418\n",
      "Episode 174, Reward: -22.40, Epsilon: 0.416\n",
      "Episode 175, Reward: -90.16, Epsilon: 0.414\n",
      "Episode 176, Reward: -65.56, Epsilon: 0.412\n",
      "Episode 177, Reward: -287.42, Epsilon: 0.410\n",
      "Episode 178, Reward: -39.49, Epsilon: 0.408\n",
      "Episode 179, Reward: 7.09, Epsilon: 0.406\n",
      "Episode 180, Reward: -101.13, Epsilon: 0.404\n",
      "Episode 181, Reward: -127.82, Epsilon: 0.402\n",
      "Episode 182, Reward: -15.45, Epsilon: 0.400\n",
      "Episode 183, Reward: 13.71, Epsilon: 0.398\n",
      "Episode 184, Reward: -81.02, Epsilon: 0.396\n",
      "Episode 185, Reward: 122.65, Epsilon: 0.394\n",
      "Episode 186, Reward: -79.99, Epsilon: 0.392\n",
      "Episode 187, Reward: -89.76, Epsilon: 0.390\n",
      "Episode 188, Reward: 14.20, Epsilon: 0.388\n",
      "Episode 189, Reward: -52.36, Epsilon: 0.386\n",
      "Episode 190, Reward: -33.11, Epsilon: 0.384\n",
      "Episode 191, Reward: 5.57, Epsilon: 0.382\n",
      "Episode 192, Reward: -17.12, Epsilon: 0.380\n",
      "Episode 193, Reward: -8.94, Epsilon: 0.378\n",
      "Episode 194, Reward: 37.80, Epsilon: 0.376\n",
      "Episode 195, Reward: -67.86, Epsilon: 0.374\n",
      "Episode 196, Reward: -6.58, Epsilon: 0.373\n",
      "Episode 197, Reward: -94.09, Epsilon: 0.371\n",
      "Episode 198, Reward: 74.64, Epsilon: 0.369\n",
      "Episode 199, Reward: -10.39, Epsilon: 0.367\n",
      "Episode 200, Reward: -53.62, Epsilon: 0.365\n",
      "Episode 201, Reward: -36.74, Epsilon: 0.363\n",
      "Episode 202, Reward: -68.97, Epsilon: 0.361\n",
      "Episode 203, Reward: 8.87, Epsilon: 0.360\n",
      "Episode 204, Reward: -15.42, Epsilon: 0.358\n",
      "Episode 205, Reward: -66.73, Epsilon: 0.356\n",
      "Episode 206, Reward: 2.16, Epsilon: 0.354\n",
      "Episode 207, Reward: -34.41, Epsilon: 0.353\n",
      "Episode 208, Reward: 9.82, Epsilon: 0.351\n",
      "Episode 209, Reward: -47.42, Epsilon: 0.349\n",
      "Episode 210, Reward: -44.30, Epsilon: 0.347\n",
      "Episode 211, Reward: -28.35, Epsilon: 0.346\n",
      "Episode 212, Reward: -49.15, Epsilon: 0.344\n",
      "Episode 213, Reward: -39.58, Epsilon: 0.342\n",
      "Episode 214, Reward: 46.94, Epsilon: 0.340\n",
      "Episode 215, Reward: 39.63, Epsilon: 0.339\n",
      "Episode 216, Reward: 22.58, Epsilon: 0.337\n",
      "Episode 217, Reward: -57.32, Epsilon: 0.335\n",
      "Episode 218, Reward: -5.92, Epsilon: 0.334\n",
      "Episode 219, Reward: -105.92, Epsilon: 0.332\n",
      "Episode 220, Reward: -76.44, Epsilon: 0.330\n",
      "Episode 221, Reward: -97.65, Epsilon: 0.329\n",
      "Episode 222, Reward: -47.71, Epsilon: 0.327\n",
      "Episode 223, Reward: -116.11, Epsilon: 0.325\n",
      "Episode 224, Reward: -166.75, Epsilon: 0.324\n",
      "Episode 225, Reward: 52.47, Epsilon: 0.322\n",
      "Episode 226, Reward: 3.87, Epsilon: 0.321\n",
      "Episode 227, Reward: -206.24, Epsilon: 0.319\n",
      "Episode 228, Reward: 85.08, Epsilon: 0.317\n",
      "Episode 229, Reward: 25.95, Epsilon: 0.316\n",
      "Episode 230, Reward: 82.25, Epsilon: 0.314\n",
      "Episode 231, Reward: 10.51, Epsilon: 0.313\n",
      "Episode 232, Reward: -25.98, Epsilon: 0.311\n",
      "Episode 233, Reward: -311.67, Epsilon: 0.309\n",
      "Episode 234, Reward: -54.28, Epsilon: 0.308\n",
      "Episode 235, Reward: -36.83, Epsilon: 0.306\n",
      "Episode 236, Reward: -180.51, Epsilon: 0.305\n",
      "Episode 237, Reward: -39.58, Epsilon: 0.303\n",
      "Episode 238, Reward: -317.88, Epsilon: 0.302\n",
      "Episode 239, Reward: 33.28, Epsilon: 0.300\n",
      "Episode 240, Reward: 75.31, Epsilon: 0.299\n",
      "Episode 241, Reward: 109.85, Epsilon: 0.297\n",
      "Episode 242, Reward: -74.31, Epsilon: 0.296\n",
      "Episode 243, Reward: 22.24, Epsilon: 0.294\n",
      "Episode 244, Reward: 120.80, Epsilon: 0.293\n",
      "Episode 245, Reward: -303.77, Epsilon: 0.291\n",
      "Episode 246, Reward: -16.57, Epsilon: 0.290\n",
      "Episode 247, Reward: 111.64, Epsilon: 0.288\n",
      "Episode 248, Reward: 171.25, Epsilon: 0.287\n",
      "Episode 249, Reward: 287.48, Epsilon: 0.286\n",
      "Episode 250, Reward: 109.28, Epsilon: 0.284\n",
      "Episode 251, Reward: 69.43, Epsilon: 0.283\n",
      "Episode 252, Reward: 16.74, Epsilon: 0.281\n",
      "Episode 253, Reward: 59.17, Epsilon: 0.280\n",
      "Episode 254, Reward: -71.79, Epsilon: 0.279\n",
      "Episode 255, Reward: 40.26, Epsilon: 0.277\n",
      "Episode 256, Reward: -10.16, Epsilon: 0.276\n",
      "Episode 257, Reward: 14.38, Epsilon: 0.274\n",
      "Episode 258, Reward: 40.60, Epsilon: 0.273\n",
      "Episode 259, Reward: -145.86, Epsilon: 0.272\n",
      "Episode 260, Reward: 52.68, Epsilon: 0.270\n",
      "Episode 261, Reward: -255.03, Epsilon: 0.269\n",
      "Episode 262, Reward: -17.39, Epsilon: 0.268\n",
      "Episode 263, Reward: 104.62, Epsilon: 0.266\n",
      "Episode 264, Reward: -234.60, Epsilon: 0.265\n",
      "Episode 265, Reward: -86.19, Epsilon: 0.264\n",
      "Episode 266, Reward: 71.26, Epsilon: 0.262\n",
      "Episode 267, Reward: -57.75, Epsilon: 0.261\n",
      "Episode 268, Reward: -119.15, Epsilon: 0.260\n",
      "Episode 269, Reward: -86.44, Epsilon: 0.258\n",
      "Episode 270, Reward: 129.02, Epsilon: 0.257\n",
      "Episode 271, Reward: -431.40, Epsilon: 0.256\n",
      "Episode 272, Reward: 243.35, Epsilon: 0.255\n",
      "Episode 273, Reward: 2.24, Epsilon: 0.253\n",
      "Episode 274, Reward: 138.27, Epsilon: 0.252\n",
      "Episode 275, Reward: -97.29, Epsilon: 0.251\n",
      "Episode 276, Reward: -65.62, Epsilon: 0.249\n",
      "Episode 277, Reward: 195.48, Epsilon: 0.248\n",
      "Episode 278, Reward: 132.92, Epsilon: 0.247\n",
      "Episode 279, Reward: -9.37, Epsilon: 0.246\n",
      "Episode 280, Reward: -94.38, Epsilon: 0.245\n",
      "Episode 281, Reward: -27.94, Epsilon: 0.243\n",
      "Episode 282, Reward: 84.50, Epsilon: 0.242\n",
      "Episode 283, Reward: -90.07, Epsilon: 0.241\n",
      "Episode 284, Reward: -67.03, Epsilon: 0.240\n",
      "Episode 285, Reward: 229.58, Epsilon: 0.238\n",
      "Episode 286, Reward: 16.94, Epsilon: 0.237\n",
      "Episode 287, Reward: 35.55, Epsilon: 0.236\n",
      "Episode 288, Reward: -73.04, Epsilon: 0.235\n",
      "Episode 289, Reward: -93.08, Epsilon: 0.234\n",
      "Episode 290, Reward: -98.39, Epsilon: 0.233\n",
      "Episode 291, Reward: -99.40, Epsilon: 0.231\n",
      "Episode 292, Reward: 108.83, Epsilon: 0.230\n",
      "Episode 293, Reward: 69.02, Epsilon: 0.229\n",
      "Episode 294, Reward: 162.77, Epsilon: 0.228\n",
      "Episode 295, Reward: -79.75, Epsilon: 0.227\n",
      "Episode 296, Reward: -17.36, Epsilon: 0.226\n",
      "Episode 297, Reward: -114.50, Epsilon: 0.225\n",
      "Episode 298, Reward: 292.90, Epsilon: 0.223\n",
      "Episode 299, Reward: -7.39, Epsilon: 0.222\n",
      "Episode 300, Reward: -3.22, Epsilon: 0.221\n",
      "Episode 301, Reward: 130.92, Epsilon: 0.220\n",
      "Episode 302, Reward: 3.91, Epsilon: 0.219\n",
      "Episode 303, Reward: 56.69, Epsilon: 0.218\n",
      "Episode 304, Reward: -146.99, Epsilon: 0.217\n",
      "Episode 305, Reward: -3.81, Epsilon: 0.216\n",
      "Episode 306, Reward: -12.39, Epsilon: 0.215\n",
      "Episode 307, Reward: -25.07, Epsilon: 0.214\n",
      "Episode 308, Reward: 142.32, Epsilon: 0.212\n",
      "Episode 309, Reward: 259.68, Epsilon: 0.211\n",
      "Episode 310, Reward: 241.63, Epsilon: 0.210\n",
      "Episode 311, Reward: 227.75, Epsilon: 0.209\n",
      "Episode 312, Reward: 96.84, Epsilon: 0.208\n",
      "Episode 313, Reward: 78.81, Epsilon: 0.207\n",
      "Episode 314, Reward: 93.11, Epsilon: 0.206\n",
      "Episode 315, Reward: 290.44, Epsilon: 0.205\n",
      "Episode 316, Reward: 101.83, Epsilon: 0.204\n",
      "Episode 317, Reward: 140.72, Epsilon: 0.203\n",
      "Episode 318, Reward: 51.16, Epsilon: 0.202\n",
      "Episode 319, Reward: 161.57, Epsilon: 0.201\n",
      "Episode 320, Reward: 235.56, Epsilon: 0.200\n",
      "Episode 321, Reward: -62.88, Epsilon: 0.199\n",
      "Episode 322, Reward: -120.68, Epsilon: 0.198\n",
      "Episode 323, Reward: 47.23, Epsilon: 0.197\n",
      "Episode 324, Reward: -40.64, Epsilon: 0.196\n",
      "Episode 325, Reward: -326.31, Epsilon: 0.195\n",
      "Episode 326, Reward: 82.06, Epsilon: 0.194\n",
      "Episode 327, Reward: 10.10, Epsilon: 0.193\n",
      "Episode 328, Reward: -62.93, Epsilon: 0.192\n",
      "Episode 329, Reward: 160.81, Epsilon: 0.191\n",
      "Episode 330, Reward: 45.08, Epsilon: 0.190\n",
      "Episode 331, Reward: 12.78, Epsilon: 0.189\n",
      "Episode 332, Reward: -60.08, Epsilon: 0.188\n",
      "Episode 333, Reward: 201.58, Epsilon: 0.187\n",
      "Episode 334, Reward: 111.29, Epsilon: 0.187\n",
      "Episode 335, Reward: 235.63, Epsilon: 0.186\n",
      "Episode 336, Reward: -62.79, Epsilon: 0.185\n",
      "Episode 337, Reward: 117.13, Epsilon: 0.184\n",
      "Episode 338, Reward: -181.56, Epsilon: 0.183\n",
      "Episode 339, Reward: -39.65, Epsilon: 0.182\n",
      "Episode 340, Reward: -104.54, Epsilon: 0.181\n",
      "Episode 341, Reward: -290.58, Epsilon: 0.180\n",
      "Episode 342, Reward: -67.86, Epsilon: 0.179\n",
      "Episode 343, Reward: -321.75, Epsilon: 0.178\n",
      "Episode 344, Reward: -1.16, Epsilon: 0.177\n",
      "Episode 345, Reward: -246.22, Epsilon: 0.177\n",
      "Episode 346, Reward: 204.80, Epsilon: 0.176\n",
      "Episode 347, Reward: -27.32, Epsilon: 0.175\n",
      "Episode 348, Reward: 94.09, Epsilon: 0.174\n",
      "Episode 349, Reward: 108.33, Epsilon: 0.173\n",
      "Episode 350, Reward: -383.66, Epsilon: 0.172\n",
      "Episode 351, Reward: -16.55, Epsilon: 0.171\n",
      "Episode 352, Reward: -48.92, Epsilon: 0.170\n",
      "Episode 353, Reward: -568.80, Epsilon: 0.170\n",
      "Episode 354, Reward: -119.11, Epsilon: 0.169\n",
      "Episode 355, Reward: 4.23, Epsilon: 0.168\n",
      "Episode 356, Reward: -209.81, Epsilon: 0.167\n",
      "Episode 357, Reward: -320.96, Epsilon: 0.166\n",
      "Episode 358, Reward: -193.40, Epsilon: 0.165\n",
      "Episode 359, Reward: -129.72, Epsilon: 0.165\n",
      "Episode 360, Reward: -476.16, Epsilon: 0.164\n",
      "Episode 361, Reward: -285.71, Epsilon: 0.163\n",
      "Episode 362, Reward: 130.67, Epsilon: 0.162\n",
      "Episode 363, Reward: 36.89, Epsilon: 0.161\n",
      "Episode 364, Reward: -382.29, Epsilon: 0.160\n",
      "Episode 365, Reward: 233.76, Epsilon: 0.160\n",
      "Episode 366, Reward: -290.05, Epsilon: 0.159\n",
      "Episode 367, Reward: -461.00, Epsilon: 0.158\n",
      "Episode 368, Reward: -389.65, Epsilon: 0.157\n",
      "Episode 369, Reward: -7.60, Epsilon: 0.157\n",
      "Episode 370, Reward: 191.03, Epsilon: 0.156\n",
      "Episode 371, Reward: -132.02, Epsilon: 0.155\n",
      "Episode 372, Reward: -171.17, Epsilon: 0.154\n",
      "Episode 373, Reward: -42.89, Epsilon: 0.153\n",
      "Episode 374, Reward: -371.14, Epsilon: 0.153\n",
      "Episode 375, Reward: -74.77, Epsilon: 0.152\n",
      "Episode 376, Reward: 12.93, Epsilon: 0.151\n",
      "Episode 377, Reward: -280.73, Epsilon: 0.150\n",
      "Episode 378, Reward: -236.63, Epsilon: 0.150\n",
      "Episode 379, Reward: -233.29, Epsilon: 0.149\n",
      "Episode 380, Reward: -273.22, Epsilon: 0.148\n",
      "Episode 381, Reward: -260.70, Epsilon: 0.147\n",
      "Episode 382, Reward: -34.72, Epsilon: 0.147\n",
      "Episode 383, Reward: -387.91, Epsilon: 0.146\n",
      "Episode 384, Reward: -317.11, Epsilon: 0.145\n",
      "Episode 385, Reward: 250.80, Epsilon: 0.144\n",
      "Episode 386, Reward: -335.12, Epsilon: 0.144\n",
      "Episode 387, Reward: 246.54, Epsilon: 0.143\n",
      "Episode 388, Reward: -13.58, Epsilon: 0.142\n",
      "Episode 389, Reward: 226.84, Epsilon: 0.142\n",
      "Episode 390, Reward: -199.98, Epsilon: 0.141\n",
      "Episode 391, Reward: -285.12, Epsilon: 0.140\n",
      "Episode 392, Reward: -223.52, Epsilon: 0.139\n",
      "Episode 393, Reward: -218.49, Epsilon: 0.139\n",
      "Episode 394, Reward: -337.84, Epsilon: 0.138\n",
      "Episode 395, Reward: 5.95, Epsilon: 0.137\n",
      "Episode 396, Reward: 208.46, Epsilon: 0.137\n",
      "Episode 397, Reward: -552.93, Epsilon: 0.136\n",
      "Episode 398, Reward: -8.07, Epsilon: 0.135\n",
      "Episode 399, Reward: -375.95, Epsilon: 0.135\n",
      "Episode 400, Reward: -263.66, Epsilon: 0.134\n",
      "Episode 401, Reward: -304.23, Epsilon: 0.133\n",
      "Episode 402, Reward: -9.87, Epsilon: 0.133\n",
      "Episode 403, Reward: -87.09, Epsilon: 0.132\n",
      "Episode 404, Reward: -376.25, Epsilon: 0.131\n",
      "Episode 405, Reward: -183.10, Epsilon: 0.131\n",
      "Episode 406, Reward: -431.79, Epsilon: 0.130\n",
      "Episode 407, Reward: -319.20, Epsilon: 0.129\n",
      "Episode 408, Reward: 238.34, Epsilon: 0.129\n",
      "Episode 409, Reward: -237.56, Epsilon: 0.128\n",
      "Episode 410, Reward: -301.54, Epsilon: 0.127\n",
      "Episode 411, Reward: -343.88, Epsilon: 0.127\n",
      "Episode 412, Reward: -308.11, Epsilon: 0.126\n",
      "Episode 413, Reward: -60.54, Epsilon: 0.126\n",
      "Episode 414, Reward: -226.76, Epsilon: 0.125\n",
      "Episode 415, Reward: -637.63, Epsilon: 0.124\n",
      "Episode 416, Reward: -132.72, Epsilon: 0.124\n",
      "Episode 417, Reward: -152.64, Epsilon: 0.123\n",
      "Episode 418, Reward: -484.29, Epsilon: 0.122\n",
      "Episode 419, Reward: -370.54, Epsilon: 0.122\n",
      "Episode 420, Reward: -243.79, Epsilon: 0.121\n",
      "Episode 421, Reward: -276.27, Epsilon: 0.121\n",
      "Episode 422, Reward: -79.51, Epsilon: 0.120\n",
      "Episode 423, Reward: -551.87, Epsilon: 0.119\n",
      "Episode 424, Reward: -1.05, Epsilon: 0.119\n",
      "Episode 425, Reward: -340.55, Epsilon: 0.118\n",
      "Episode 426, Reward: -199.12, Epsilon: 0.118\n",
      "Episode 427, Reward: -424.85, Epsilon: 0.117\n",
      "Episode 428, Reward: -241.80, Epsilon: 0.116\n",
      "Episode 429, Reward: -322.00, Epsilon: 0.116\n",
      "Episode 430, Reward: -593.76, Epsilon: 0.115\n",
      "Episode 431, Reward: -277.19, Epsilon: 0.115\n",
      "Episode 432, Reward: -358.90, Epsilon: 0.114\n",
      "Episode 433, Reward: -204.50, Epsilon: 0.114\n",
      "Episode 434, Reward: -141.12, Epsilon: 0.113\n",
      "Episode 435, Reward: -280.02, Epsilon: 0.112\n",
      "Episode 436, Reward: -226.52, Epsilon: 0.112\n",
      "Episode 437, Reward: 26.01, Epsilon: 0.111\n",
      "Episode 438, Reward: 263.89, Epsilon: 0.111\n",
      "Episode 439, Reward: 251.31, Epsilon: 0.110\n",
      "Episode 440, Reward: -316.77, Epsilon: 0.110\n",
      "Episode 441, Reward: -526.18, Epsilon: 0.109\n",
      "Episode 442, Reward: 30.31, Epsilon: 0.109\n",
      "Episode 443, Reward: -136.14, Epsilon: 0.108\n",
      "Episode 444, Reward: -100.90, Epsilon: 0.107\n",
      "Episode 445, Reward: -127.14, Epsilon: 0.107\n",
      "Episode 446, Reward: -271.16, Epsilon: 0.106\n",
      "Episode 447, Reward: -368.98, Epsilon: 0.106\n",
      "Episode 448, Reward: -276.77, Epsilon: 0.105\n",
      "Episode 449, Reward: -194.09, Epsilon: 0.105\n",
      "Episode 450, Reward: 254.48, Epsilon: 0.104\n",
      "Episode 451, Reward: -96.36, Epsilon: 0.104\n",
      "Episode 452, Reward: -268.10, Epsilon: 0.103\n",
      "Episode 453, Reward: -69.37, Epsilon: 0.103\n",
      "Episode 454, Reward: -259.09, Epsilon: 0.102\n",
      "Episode 455, Reward: -79.69, Epsilon: 0.102\n",
      "Episode 456, Reward: 251.54, Epsilon: 0.101\n",
      "Episode 457, Reward: 88.92, Epsilon: 0.101\n",
      "Episode 458, Reward: -134.68, Epsilon: 0.100\n",
      "Episode 459, Reward: -423.64, Epsilon: 0.100\n",
      "Episode 460, Reward: -414.07, Epsilon: 0.099\n",
      "Episode 461, Reward: -339.73, Epsilon: 0.099\n",
      "Episode 462, Reward: -323.59, Epsilon: 0.098\n",
      "Episode 463, Reward: -109.98, Epsilon: 0.098\n",
      "Episode 464, Reward: -245.80, Epsilon: 0.097\n",
      "Episode 465, Reward: -301.69, Epsilon: 0.097\n",
      "Episode 466, Reward: -178.03, Epsilon: 0.096\n",
      "Episode 467, Reward: -281.13, Epsilon: 0.096\n",
      "Episode 468, Reward: -318.76, Epsilon: 0.095\n",
      "Episode 469, Reward: -336.30, Epsilon: 0.095\n",
      "Episode 470, Reward: -389.59, Epsilon: 0.094\n",
      "Episode 471, Reward: -221.86, Epsilon: 0.094\n",
      "Episode 472, Reward: -199.51, Epsilon: 0.093\n",
      "Episode 473, Reward: -432.51, Epsilon: 0.093\n",
      "Episode 474, Reward: -114.19, Epsilon: 0.092\n",
      "Episode 475, Reward: -337.12, Epsilon: 0.092\n",
      "Episode 476, Reward: -216.79, Epsilon: 0.092\n",
      "Episode 477, Reward: -293.42, Epsilon: 0.091\n",
      "Episode 478, Reward: -195.11, Epsilon: 0.091\n",
      "Episode 479, Reward: -234.14, Epsilon: 0.090\n",
      "Episode 480, Reward: -304.07, Epsilon: 0.090\n",
      "Episode 481, Reward: -233.32, Epsilon: 0.089\n",
      "Episode 482, Reward: -373.48, Epsilon: 0.089\n",
      "Episode 483, Reward: -626.52, Epsilon: 0.088\n",
      "Episode 484, Reward: -254.02, Epsilon: 0.088\n",
      "Episode 485, Reward: -407.63, Epsilon: 0.088\n",
      "Episode 486, Reward: -162.68, Epsilon: 0.087\n",
      "Episode 487, Reward: -479.50, Epsilon: 0.087\n",
      "Episode 488, Reward: -250.34, Epsilon: 0.086\n",
      "Episode 489, Reward: -222.20, Epsilon: 0.086\n",
      "Episode 490, Reward: -260.15, Epsilon: 0.085\n",
      "Episode 491, Reward: -312.05, Epsilon: 0.085\n",
      "Episode 492, Reward: -256.16, Epsilon: 0.084\n",
      "Episode 493, Reward: -267.63, Epsilon: 0.084\n",
      "Episode 494, Reward: -486.29, Epsilon: 0.084\n",
      "Episode 495, Reward: -62.39, Epsilon: 0.083\n",
      "Episode 496, Reward: -345.06, Epsilon: 0.083\n",
      "Episode 497, Reward: -321.86, Epsilon: 0.082\n",
      "Episode 498, Reward: -662.10, Epsilon: 0.082\n",
      "Episode 499, Reward: -385.61, Epsilon: 0.082\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 500\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 1000  # steps\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track total steps\n",
    "total_steps = 0\n",
    "\n",
    "# For logging\n",
    "episode_rewards = []\n",
    "epsilon_history = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Select action using epsilon-greedy\n",
    "        action = select_action(state, q_net, epsilon, action_dim)\n",
    "\n",
    "        # Interact with environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        # Start training only after batch is ready\n",
    "        if len(buffer) >= batch_size:\n",
    "            # Sample mini-batch\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "            # Compute Q-values for current states\n",
    "            q_values = q_net(states).gather(1, actions)\n",
    "\n",
    "            # Compute max Q-values for next states from target network\n",
    "            with torch.no_grad():\n",
    "                max_next_q = target_q_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                q_targets = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.mse_loss(q_values, q_targets)\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Periodically update target network\n",
    "        if total_steps % target_update_freq == 0:\n",
    "            target_q_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Log metrics\n",
    "    episode_rewards.append(total_reward)\n",
    "    epsilon_history.append(epsilon)\n",
    "    print(f\"Episode {episode}, Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "\n",
    "# Save episode rewards to a file\n",
    "with open(\"episode_rewards.txt\", \"w\") as f:\n",
    "    for r in episode_rewards:\n",
    "        f.write(f\"{r}\\n\")\n",
    "\n",
    "# Save epsilon history to a file\n",
    "with open(\"epsilon_history.txt\", \"w\") as f:\n",
    "    for eps in epsilon_history:\n",
    "        f.write(f\"{eps}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e9836",
   "metadata": {},
   "source": [
    "## 2.6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596b4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load episode rewards\n",
    "with open(\"episode_rewards.txt\", \"r\") as f:\n",
    "    episode_rewards = [float(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Load epsilon history\n",
    "with open(\"epsilon_history.txt\", \"r\") as f:\n",
    "    epsilon_history = [float(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Create episode index\n",
    "episodes = np.arange(len(episode_rewards))\n",
    "\n",
    "# Compute moving average of rewards\n",
    "window = 20\n",
    "moving_avg = pd.Series(episode_rewards).rolling(window).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a993a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Use a non-interactive backend\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed24e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Plot 1: Reward per episode + moving average\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(episodes, episode_rewards, label=\"Reward per Episode\", alpha=0.4)\n",
    "plt.plot(episodes, moving_avg, label=f\"{window}-Episode Moving Average\", color='red', linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Performance on LunarLander-v2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4943296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Epsilon decay over time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(episodes, epsilon_history, label=\"Epsilon\", color='purple')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.title(\"Exploration Rate (Epsilon) Over Time\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Histogram of reward distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(episode_rewards, bins=30, edgecolor='black')\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Reward Distribution Across Episodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fe8ae",
   "metadata": {},
   "source": [
    "# 3. Sarsa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
